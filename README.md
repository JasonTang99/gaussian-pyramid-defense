# Gaussian Pyramid Ensemble Defense against White-box Adversarial Attacks 

For a more detailed description of the project, please see the [final report](report/report.pdf).

## Summary
In the realm of machine learning, particularly computer vision, adversarial attacks pose significant security risks. This project introduces GPEnsemble, a novel defense system, and evaluates its robustness against white-box adversarial attacks. The proposed system combines a gaussian pyramid-inspired ensemble defense with an adversarially trained DnCNN denoiser preprocessor. The project focuses on enhancing model integrity against attacks that generate imperceptibly small perturbations to images, causing erroneous model predictions.

<!-- diagram -->
#### Diagram of Proposed Method
![Alt text](report/images/diagram.png?raw=true "GPEnsemble Diagram")

### Gaussian Pyramid Ensemble
GPEnsemble employs an ensemble of models, each receiving differently resized input images. The resizing is performed using scaling factors such as 2.0 and 1.1. The motivation is to create a diverse set of models that are difficult to simultaneously fool. The ensemble's outputs are combined using uniform and weighted averaging, as well as majority voting.

### Denoising Preprocessor
The system includes a DnCNN denoiser in preprocessing. This denoiser is trained with adversarial examples generated by various attack methods, enhancing image restoration and model robustness.

## Evaluation and Results
We use the library [cleverhans 4.0.0](https://github.com/cleverhans-lab/cleverhans/releases/tag/v4.0.0) for our attack implementations. The proposed defense system is evaluated against FGSM, PGD, and C&W attacks on MNIST and CIFAR10 datasets. Various ensemble configurations and denoising methods are analyzed. The results demonstrate that GPEnsemble, particularly when combined with the DnCNN denoiser, outperforms existing defense systems, achieving improved accuracy under strong attacks.

<!-- voting_methods -->
#### Results on different Voting Methods
![Alt text](report/images/voting_methods.png?raw=true "Voting Methods")

<!-- denoiser_only_result -->
<!-- #### Results on Denoiser Only
![Alt text](report/images/denoiser_only_result.png?raw=true "Denoiser Only") -->

<!-- denoiser_sample -->
#### Denoiser Samples
![Alt text](report/images/denoiser_sample.png?raw=true "Denoiser Sample")

#### Results
|   dataset  |  attack  |  scaling, up, down  |  test_acc   |
|:----------:|:--------:|:-------------------:|:-----------:|
|   mnist    |   fgsm   |      1.1, 3, 3      |   98.44%    |
|            |          |      1.1, 5, 5      |   98.65%    |
|            |          |      1.1, 7, 7      |   98.79%    |
|            |          |      2.0, 3, 3      | **99.01%**  |
|            |   pgd    |      1.1, 3, 3      |   98.65%    |
|            |          |      1.1, 5, 5      |   98.70%    |
|            |          |      1.1, 7, 7      |   98.62%    |
|            |          |      2.0, 3, 3      | **99.10%**  |
|            |    cw    |      1.1, 3, 3      |   67.20%    |
|            |          |      1.1, 5, 5      |   75.99%    |
|            |          |      1.1, 7, 7      |   77.09%    |
|            |          |      2.0, 3, 3      | **81.15%**  |
|  cifar10   |   fgsm   |      1.1, 3, 3      |   43.25%    |
|            |          |      1.1, 5, 5      |   43.93%    |
|            |          |      1.1, 7, 7      | **45.25%**  |
|            |          |      2.0, 3, 3      | **45.48%**  |
|            |   pgd    |      1.1, 3, 3      |   47.78%    |
|            |          |      1.1, 5, 5      |   49.89%    |
|            |          |      1.1, 7, 7      | **51.86%**  |
|            |          |      2.0, 3, 3      |   40.62%    |
|            |    cw    |      1.1, 3, 3      |   37.91%    |
|            |          |      1.1, 5, 5      |   45.95%    |
|            |          |      1.1, 7, 7      | **47.17%**  |
|            |          |      2.0, 3, 3      | **46.98%**  |


<!-- comparison -->
#### Comparison with Ensemble Adversarial Training (EnsAdv) and Fast Adversarial Training
![Alt text](report/images/comparison.png?raw=true "Comparison")


## Conclusions and Future Work
GPEnsemble showcases promising results in bolstering model robustness against adversarial attacks. Future directions include extending the approach to larger datasets and further exploring non-differentiable techniques.


## Code Organization:

### Ensemble + Attacks
- ```attack_results/```: storage for all experimental results in pkl files.
- ```cleverhans_fixed/```: modified PGD implementation to fix a memory leak bug.
- ```comparison_defenses/```: copies of implementations of [Ensemble Adversarial Training](https://github.com/JZ-LIANG/Ensemble-Adversarial-Training) and [Fast Adversarial Training](https://github.com/locuslab/fast_adversarial).
- ```graphs/```: holds graphs for use in the report.
- ```models/```: defines our custom denoisers, GPEnsemble, and Resnet18 for use in other functions.
- ```trained_models/```: holds trained ```.pth``` models as well as pickle files with validation set accuracies for use in the weighted voting functions in GPEnsemble. Also holds trained models for EnsAdv and FastAdv for us to compare with.
- ```attack.py```: loads test data and evaluates given models against FGSM, PGD and CW attacks.
- ```datasets.py```: provides dataloaders to other functions.
- ```parse_args.py```: provides command line argument parsing capability to other functions.
- ```plot_exp.ipynb```: Jupyter Notebook for processing all the experimental data and producing ensemble and ensemble+denoiser graphs.
- ```run_comparison_experiments.py```: runs all attacks on compared methods (EnsAdv and FastAdv).
- ```run_ensemble_experiments.py```: runs all attacks on a variety of hyperparameter settings for our GPEnsemble method (also includes denoiser+ensemble evaluation)
- ```run_model_training.py```: trains all the prerequisite ensemble members for later use in GPEnsemble.
- ```requirements.txt```: pip packages needed to run all code.

Sample Usage:
```
python run_model_training.py
python run_ensemble_experiments.py
python run_comparison_experiments.py
```

### Denoiser
- ```trained_denoisers/```: holds pretrained denoiser models.
- ```models/denoisers.py```: defines denoiser architectures.
- ```adversarial_dataset.py```: loads and generates custom MNIST and CIFAR10 adversarial dataset for training.
- ```test_denoiser.py```: test denoiser performance by evaluating model accuracy, PSNR, and SSIM of reconstructed images. Also generate FGSM, PGD and CW attacks.
- ```plot_denoiser_exp.ipynb```: plot visualization and results on denoisers
- ```train_denoiser.py```: trains the denoiser models with user-specified parameters. 

Sample Usage:
```
python train_denoiser.py --dataset=cifar10 --arch=dncnn --lr=1e-3 --batch_size=64 --epochs=5
```
